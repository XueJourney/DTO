const express = require('express');
const cors = require('cors');
const fetch = require('node-fetch');

const app = express();
const PORT = process.env.PORT || 3000;

// ä¸­é—´ä»¶
app.use(cors());
app.use(express.json());

// é…ç½®ä¿¡æ¯
const CONFIG = {
  API_URL: 'https://www.gpt4novel.com/api/xiaoshuoai/ext/v1/chat/completions',
  DEFAULT_MODEL: 'nalang-xl-10',
  DEFAULT_TEMPERATURE: 0.7,
  DEFAULT_MAX_TOKENS: 800,
  DEFAULT_TOP_P: 0.35,
  DEFAULT_REPETITION_PENALTY: 1.05
};

// æ¨¡å‹æ˜ å°„ - å°†OpenAIæ¨¡å‹åç§°æ˜ å°„åˆ°APIæ”¯æŒçš„æ¨¡å‹
const MODEL_MAPPING = {
  'gpt-3.5-turbo': 'nalang-turbo-v19',
  'gpt-3.5-turbo-16k': 'nalang-xl-16k',
  'gpt-4': 'nalang-xl-10',
  'gpt-4-32k': 'nalang-xl-16k',
  'gpt-4-turbo': 'nalang-v17-2',
  'gpt-4-turbo-preview': 'nalang-v17-2',
  'nalang-turbo-v19':'nalang-turbo-v19',
  'nalang-xl-16k':'nalang-xl-16k',
  'nalang-xl-10':'nalang-xl-10',
  'nalang-v17-2':'nalang-v17-2'
};

// ä»è¯·æ±‚ä¸­æå–APIå¯†é’¥
function extractApiKey(req) {
  const authHeader = req.headers.authorization;
  if (authHeader && authHeader.startsWith('Bearer ')) {
    return authHeader.slice(7);
  }
  return null;
}

// è§£ææµå¼å“åº”æ•°æ®
function parseStreamLine(line) {
  if (!line.startsWith('data: ')) {
    return null;
  }
  
  try {
    const jsonStr = line.slice(6).trim();
    if (jsonStr === '[DONE]') {
      return { done: true };
    }
    
    const jsonData = JSON.parse(jsonStr);
    return jsonData;
  } catch (e) {
    return null;
  }
}

// å¤„ç†æµå¼å“åº”å¹¶æ”¶é›†å®Œæ•´å†…å®¹
function collectStreamContent(response) {
  return new Promise((resolve, reject) => {
    let fullContent = '';
    const decoder = new TextDecoder();
    let buffer = '';

    response.body.on('data', (chunk) => {
      buffer += decoder.decode(chunk, { stream: true });
      
      let newlineIndex;
      while ((newlineIndex = buffer.indexOf('\n')) !== -1) {
        const line = buffer.slice(0, newlineIndex);
        buffer = buffer.slice(newlineIndex + 1);
        
        const parsed = parseStreamLine(line);
        if (parsed) {
          if (parsed.done) {
            resolve(fullContent);
            return;
          }
          
          // å¤„ç†å†…å®¹ - æ”¯æŒä¸åŒçš„æ•°æ®æ ¼å¼
          if (parsed.choices?.[0]?.delta?.content) {
            fullContent += parsed.choices[0].delta.content;
          } else if (parsed.choices?.[0]?.message?.content) {
            fullContent += parsed.choices[0].message.content;
          }
        }
      }
    });

    response.body.on('end', () => {
      // å¤„ç†å‰©ä½™çš„buffer
      if (buffer.trim()) {
        const parsed = parseStreamLine(buffer.trim());
        if (parsed && parsed.choices?.[0]?.delta?.content) {
          fullContent += parsed.choices[0].delta.content;
        }
      }
      resolve(fullContent);
    });

    response.body.on('error', (error) => {
      reject(error);
    });
  });
}

// å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚çš„æ ¸å¿ƒå‡½æ•°
async function handleChatCompletion(req, res) {
  try {
    const apiKey = extractApiKey(req);
    if (!apiKey) {
      return res.status(401).json({
        error: {
          message: 'Missing API key. Please provide Authorization header with Bearer token.',
          type: 'authentication_error'
        }
      });
    }

    // æå–è¯·æ±‚å‚æ•°ï¼Œåªå¤„ç†åŸå§‹APIæ”¯æŒçš„å‚æ•°ï¼Œå…¶ä»–å‚æ•°å¿½ç•¥
    const {
      model = CONFIG.DEFAULT_MODEL,
      messages = [],
      stream = false,
      temperature = CONFIG.DEFAULT_TEMPERATURE,
      max_tokens = CONFIG.DEFAULT_MAX_TOKENS,
      top_p = CONFIG.DEFAULT_TOP_P,
      repetition_penalty = CONFIG.DEFAULT_REPETITION_PENALTY,
      // ä»¥ä¸‹å‚æ•°è¢«æ¥å—ä½†å¿½ç•¥ï¼ˆOpenAIæ ‡å‡†å‚æ•°ï¼‰
      frequency_penalty,
      presence_penalty,
      stop,
      n,
      logit_bias,
      user,
      response_format,
      seed,
      tools,
      tool_choice,
      ...otherParams
    } = req.body;

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!messages || !Array.isArray(messages) || messages.length === 0) {
      return res.status(400).json({
        error: {
          message: 'Messages parameter is required and must be a non-empty array.',
          type: 'invalid_request_error'
        }
      });
    }

    // æ˜ å°„æ¨¡å‹åç§°
    const mappedModel = MODEL_MAPPING[model] || model;

    // æ„å»ºè¯·æ±‚ä½“ - å¼ºåˆ¶ä½¿ç”¨æµå¼ä¼ è¾“ï¼Œå› ä¸ºç¬¬ä¸‰æ–¹APIåªæ”¯æŒæµå¼
    const requestBody = {
      model: mappedModel,
      messages: messages,
      stream: true, // å¼ºåˆ¶æµå¼ä¼ è¾“
      temperature: temperature,
      max_tokens: max_tokens,
      top_p: top_p,
      repetition_penalty: repetition_penalty
    };

    console.log(`[${new Date().toISOString()}] API Request:`, {
      model: mappedModel,
      messagesCount: messages.length,
      requestedStream: stream,
      actualStream: true, // å®é™…æ€»æ˜¯ä½¿ç”¨æµå¼
      temperature: temperature,
      max_tokens: max_tokens,
      ignoredParams: Object.keys({ frequency_penalty, presence_penalty, stop, n, logit_bias, user, response_format, seed, tools, tool_choice }).filter(key => req.body[key] !== undefined)
    });

    // å‘é€è¯·æ±‚åˆ°ç›®æ ‡API
    const response = await fetch(CONFIG.API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`[${new Date().toISOString()}] API Error:`, response.status, errorText);
      
      return res.status(response.status).json({
        error: {
          message: `API request failed: ${response.status} ${response.statusText}`,
          type: 'api_error',
          details: errorText
        }
      });
    }

    // å¤„ç†æµå¼å“åº”
    if (stream) {
      // è®¾ç½®SSEå“åº”å¤´
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');
      res.setHeader('Access-Control-Allow-Origin', '*');
      res.setHeader('Access-Control-Allow-Headers', 'Cache-Control');

      const decoder = new TextDecoder();
      let buffer = '';
      let isFirstChunk = true;
      let hasEnded = false;

      // å¤„ç†å“åº”æµ
      response.body.on('data', (chunk) => {
        if (hasEnded) return;
        
        buffer += decoder.decode(chunk, { stream: true });

        let newlineIndex;
        while ((newlineIndex = buffer.indexOf('\n')) !== -1) {
          const line = buffer.slice(0, newlineIndex);
          buffer = buffer.slice(newlineIndex + 1);

          const parsed = parseStreamLine(line);
          if (parsed) {
            if (parsed.done) {
              if (!hasEnded) {
                // å‘é€ç»“æŸæ ‡è®°
                const finishFormat = {
                  id: `chatcmpl-${Date.now()}`,
                  object: 'chat.completion.chunk',
                  created: Math.floor(Date.now() / 1000),
                  model: model,
                  choices: [{
                    index: 0,
                    delta: {},
                    finish_reason: 'stop'
                  }]
                };
                res.write(`data: ${JSON.stringify(finishFormat)}\n\n`);
                res.write('data: [DONE]\n\n');
                hasEnded = true;
                res.end();
              }
              return;
            }
            
            // å¤„ç†å†…å®¹å—
            const content = parsed.choices?.[0]?.delta?.content || parsed.choices?.[0]?.message?.content;
            if (content && !hasEnded) {
              const openaiFormat = {
                id: `chatcmpl-${Date.now()}`,
                object: 'chat.completion.chunk',
                created: Math.floor(Date.now() / 1000),
                model: model,
                choices: [{
                  index: 0,
                  delta: {
                    role: isFirstChunk ? 'assistant' : undefined,
                    content: content
                  },
                  finish_reason: null
                }]
              };
              
              res.write(`data: ${JSON.stringify(openaiFormat)}\n\n`);
              isFirstChunk = false;
            }
          }
        }
      });

      response.body.on('end', () => {
        if (hasEnded) return;
        
        // å¤„ç†å‰©ä½™æ•°æ®
        if (buffer.trim()) {
          const parsed = parseStreamLine(buffer.trim());
          if (parsed && parsed.choices?.[0]?.delta?.content) {
            const openaiFormat = {
              id: `chatcmpl-${Date.now()}`,
              object: 'chat.completion.chunk',
              created: Math.floor(Date.now() / 1000),
              model: model,
              choices: [{
                index: 0,
                delta: {
                  content: parsed.choices[0].delta.content
                },
                finish_reason: null
              }]
            };
            res.write(`data: ${JSON.stringify(openaiFormat)}\n\n`);
          }
        }
        
        if (!hasEnded) {
          // å‘é€æœ€ç»ˆç»“æŸæ ‡è®°
          const finalFormat = {
            id: `chatcmpl-${Date.now()}`,
            object: 'chat.completion.chunk',
            created: Math.floor(Date.now() / 1000),
            model: model,
            choices: [{
              index: 0,
              delta: {},
              finish_reason: 'stop'
            }]
          };
          res.write(`data: ${JSON.stringify(finalFormat)}\n\n`);
          res.write('data: [DONE]\n\n');
          hasEnded = true;
          res.end();
        }
      });

      response.body.on('error', (error) => {
        if (!hasEnded) {
          console.error('Stream error:', error);
          res.write(`data: ${JSON.stringify({ error: { message: error.message, type: 'stream_error' } })}\n\n`);
          hasEnded = true;
          res.end();
        }
      });

      // å¤„ç†å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
      req.on('close', () => {
        if (!hasEnded) {
          hasEnded = true;
          response.body.destroy();
        }
      });

    } else {
      // å¤„ç†éæµå¼å“åº” - æ”¶é›†å®Œæ•´çš„æµå¼å“åº”å†…å®¹
      try {
        const fullContent = await collectStreamContent(response);
        
        // è¿”å›OpenAIæ ¼å¼çš„å“åº”
        const openaiResponse = {
          id: `chatcmpl-${Date.now()}`,
          object: 'chat.completion',
          created: Math.floor(Date.now() / 1000),
          model: model,
          choices: [{
            index: 0,
            message: {
              role: 'assistant',
              content: fullContent
            },
            finish_reason: 'stop'
          }],
          usage: {
            prompt_tokens: 0,
            completion_tokens: fullContent.length,
            total_tokens: fullContent.length
          }
        };

        res.json(openaiResponse);
      } catch (streamError) {
        console.error('Error collecting stream content:', streamError);
        res.status(500).json({
          error: {
            message: 'Error processing stream response',
            type: 'server_error',
            details: streamError.message
          }
        });
      }
    }

  } catch (error) {
    console.error('Error in handleChatCompletion:', error);
    res.status(500).json({
      error: {
        message: 'Internal server error',
        type: 'server_error',
        details: error.message
      }
    });
  }
}

// POST è·¯ç”± - ä¸»è¦çš„èŠå¤©å®Œæˆæ¥å£
app.post('/v1/chat/completions', handleChatCompletion);

// GET è·¯ç”± - æ”¯æŒæŸ¥è¯¢å‚æ•°çš„ç®€å•èŠå¤©æ¥å£
app.get('/v1/chat/completions', (req, res) => {
  const { message, model = 'gpt-3.5-turbo', stream = 'false' } = req.query;
  
  if (!message) {
    return res.status(400).json({
      error: {
        message: 'Message parameter is required for GET requests.',
        type: 'invalid_request_error'
      }
    });
  }

  // å°†GETè¯·æ±‚è½¬æ¢ä¸ºPOSTè¯·æ±‚æ ¼å¼
  req.body = {
    model: model,
    messages: [
      { role: 'user', content: message }
    ],
    stream: stream === 'true'
  };

  handleChatCompletion(req, res);
});

// æ¨¡å‹åˆ—è¡¨æ¥å£
app.get('/v1/models', (req, res) => {
  const models = Object.keys(MODEL_MAPPING).map(key => ({
    id: key,
    object: 'model',
    created: Math.floor(Date.now() / 1000),
    owned_by: 'openai-proxy'
  }));

  res.json({
    object: 'list',
    data: models
  });
});

// å¥åº·æ£€æŸ¥æ¥å£
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    uptime: process.uptime()
  });
});

// æ ¹è·¯å¾„æä¾›APIæ–‡æ¡£
app.get('/', (req, res) => {
  res.json({
    name: 'OpenAI API Proxy',
    version: '1.0.0',
    description: 'A proxy service that converts third-party AI API to OpenAI-compatible format',
    note: 'This proxy converts streaming-only API to support both streaming and non-streaming requests',
    endpoints: {
      'POST /v1/chat/completions': 'Chat completions (OpenAI format)',
      'GET /v1/chat/completions': 'Simple chat with query parameters (?message=...)',
      'GET /v1/models': 'List available models',
      'GET /health': 'Health check'
    },
    usage: {
      authentication: 'Bearer token in Authorization header',
      example_curl: `curl -X POST ${req.protocol}://${req.get('host')}/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -H "Authorization: Bearer YOUR_API_KEY" \\
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": false
  }'`
    }
  });
});

// é”™è¯¯å¤„ç†ä¸­é—´ä»¶
app.use((err, req, res, next) => {
  console.error('Unhandled error:', err);
  res.status(500).json({
    error: {
      message: 'Internal server error',
      type: 'server_error'
    }
  });
});

// å¯åŠ¨æœåŠ¡å™¨
app.listen(PORT, () => {
  console.log(`ğŸš€ OpenAI API Proxy Server running on port ${PORT}`);
  console.log(`ğŸ“– API Documentation: http://localhost:${PORT}`);
  console.log(`ğŸ¥ Health Check: http://localhost:${PORT}/health`);
  console.log(`ğŸ“ Models List: http://localhost:${PORT}/v1/models`);
  console.log(`âš ï¸  Note: Third-party API only supports streaming, proxy converts to both formats`);
});

module.exports = app;